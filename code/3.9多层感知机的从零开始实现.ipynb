{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9多层感知机的从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\".data/dive\")\n",
    "import d2lzh_pytorch as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.1获取和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root='data/dive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.2 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "# 输入个数，输出个数，隐藏层的输入个数\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.3定义激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))# X和Tensor(0)比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.4定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.view((-1, num_inputs)) # 将每张原始图像改成长度为num_inputs的向量\n",
    "    H = relu(torch.matmul(X, W1) + b1)\n",
    "    return torch.matmul(H, W2) + b2\n",
    "    # torch.matmul：张量相乘\n",
    "    # torch.mm：矩阵相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.5定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.6训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0030, train acc 0.714, test acc 0.787\n",
      "epoch 2, loss 0.0019, train acc 0.825, test acc 0.804\n",
      "epoch 3, loss 0.0017, train acc 0.844, test acc 0.830\n",
      "epoch 4, loss 0.0015, train acc 0.856, test acc 0.817\n",
      "epoch 5, loss 0.0014, train acc 0.865, test acc 0.840\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 100.0\n",
    "# lr = 100.0：为和原书中MXNET比较，原书框架下反向传播是沿batch维求和，PyTorch是求平均\n",
    "# 相差大概batch_size倍，这里学习率调成原书的batch_size倍\n",
    "# def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "#               params=None, lr=None, optimizer=None):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "#         for X, y in train_iter:\n",
    "#             y_hat = net(X)\n",
    "#             l = loss(y_hat, y).sum()\n",
    "#             \n",
    "#             # 梯度清零\n",
    "#             if optimizer is not None:\n",
    "#                 optimizer.zero_grad()\n",
    "#             elif params is not None and params[0].grad is not None:\n",
    "#                 for param in params:\n",
    "#                     param.grad.data.zero_()\n",
    "#            \n",
    "#             l.backward()\n",
    "#             if optimizer is None:\n",
    "#                 d2l.sgd(params, lr, batch_size)\n",
    "#             else:\n",
    "#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "#             \n",
    "#             \n",
    "#             train_l_sum += l.item()\n",
    "#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "#             n += y.shape[0]\n",
    "#         test_acc = evaluate_accuracy(test_iter, net)\n",
    "#         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
